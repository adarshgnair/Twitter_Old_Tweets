# Twitter_Old_Tweets
Old_Tweets

from selenium import webdriver
from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, WebDriverException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd

class wait_for_more_than_n_elements_to_be_present(object):
    def __init__(self, locator, count):
        self.locator = locator
        self.count = count

    def __call__(self, driver):
        try:
            elements = EC._find_elements(driver, self.locator)
            return len(elements) > self.count
        except StaleElementReferenceException:
            return False

try:
    url = "https://twitter.com/search?q=Citibank%20mortgage&src=typd" #urls with appropriate keywords for tweets
    driver = webdriver.Chrome('C:\WinPython32\WinPython\chromedriver_win32\chromedriver.exe')
    driver.maximize_window()
    driver.get(url)
    # wait for some time for the tweets to load
    wait = WebDriverWait(driver, 10)
    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "li[data-item-id]")))
    # scroll down to the last tweet until there is no more tweets loaded
    while True:
        tweets = driver.find_elements_by_css_selector("li[data-item-id]")
        number_of_tweets = len(tweets)
        
        driver.execute_script("arguments[0].scrollIntoView();", tweets[-1])
        page_source = driver.page_source
        
        try:
            wait.until(wait_for_more_than_n_elements_to_be_present((By.CSS_SELECTOR, "li[data-item-id]"), number_of_tweets))
        except TimeoutException:
            driver.close()
            break
except WebDriverException:
      soup = BeautifulSoup(page_source)
      links=soup.findAll("div" ,{"class":"content"})
      a=[]
      b=[]
      c=[]
    
      for i in links:
          try:
              a.append(i.find('span', {'class':'username js-action-profile-name'}).getText())
          except AttributeError:
              a.append(' ')
          try:
              b.append(i.find(attrs={'class':'tweet-timestamp js-permalink js-nav js-tooltip'})['title'])
          except AttributeError:
              b.append(' ')
          try:
              c.append(i.find('p', {'lang':'en'}, {'class':'TweetTextSize  js-tweet-text tweet-text'}).getText())
          except AttributeError:
              c.append(' ')
        
      dataframe = pd.DataFrame({'username':a,'time_date':b,'tweet':c})
      dataframe.to_csv(r"mortgage.csv",sep=',')  #Save as csv
